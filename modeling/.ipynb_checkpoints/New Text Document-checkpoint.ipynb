{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Price Prediction Model\n",
    "\n",
    "This notebook demonstrates a comprehensive approach to building a housing price prediction model using various machine learning techniques. We'll walk through data preprocessing, feature engineering, model building, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "First, let's load the dataset and explore its basic characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('Blues_r')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('menzli_modeling.csv')\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values count:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics for numeric columns:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Plot the price distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['price'], kde=True)\n",
    "plt.title('Price Distribution')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(np.log1p(df['price']), kde=True)\n",
    "plt.title('Log Price Distribution')\n",
    "plt.xlabel('Log(Price+1)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot price vs. key features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Price vs bedrooms\n",
    "sns.boxplot(x='bedrooms', y='price', data=df, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Price by Number of Bedrooms')\n",
    "axes[0, 0].set_ylabel('Price')\n",
    "\n",
    "# Price vs is_house\n",
    "sns.boxplot(x='is_house', y='price', data=df, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Price by Property Type')\n",
    "axes[0, 1].set_xlabel('Is House (1 = House, 0 = Apartment)')\n",
    "axes[0, 1].set_ylabel('Price')\n",
    "\n",
    "# Price vs living area\n",
    "sns.scatterplot(x='living_area', y='price', data=df, alpha=0.5, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Price vs Living Area')\n",
    "axes[1, 0].set_xlabel('Living Area (sqm)')\n",
    "axes[1, 0].set_ylabel('Price')\n",
    "\n",
    "# Price vs bathrooms\n",
    "sns.boxplot(x='bathrooms', y='price', data=df, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Price by Number of Bathrooms')\n",
    "axes[1, 1].set_ylabel('Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Insights:\n",
    "- The dataset contains 8,774 properties with 32 columns\n",
    "- Price distribution is right-skewed, suggesting log transformation would be appropriate\n",
    "- There's a clear relationship between price and property type (house vs apartment)\n",
    "- Living area shows a strong positive correlation with price\n",
    "- Properties with more bedrooms and bathrooms tend to have higher prices\n",
    "- Some outliers exist in the price distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Let's preprocess the data to prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in df_processed.columns:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            if df_processed[col].dtype == 'object':\n",
    "                df_processed[col].fillna('Unknown', inplace=True)\n",
    "            else:\n",
    "                df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    \n",
    "    # Remove property_type since we have is_house column\n",
    "    if 'property_type' in df_processed.columns:\n",
    "        df_processed.drop('property_type', axis=1, inplace=True)\n",
    "    \n",
    "    # Handle outliers in feature variables (not price)\n",
    "    for col in ['living_area', 'land_area', 'bedrooms', 'bathrooms', 'total_rooms']:\n",
    "        if col in df_processed.columns:\n",
    "            q1 = df_processed[col].quantile(0.01)\n",
    "            q3 = df_processed[col].quantile(0.99)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = max(0, q1 - 1.5 * iqr)  # Ensure non-negative values\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            df_processed[col] = df_processed[col].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Preprocess data\n",
    "print(\"Preprocessing data...\")\n",
    "df_processed = preprocess_data(df)\n",
    "\n",
    "# Compare before and after\n",
    "print(\"\\nBefore preprocessing:\")\n",
    "display(df[['living_area', 'land_area', 'bedrooms', 'bathrooms', 'total_rooms']].describe())\n",
    "\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "display(df_processed[['living_area', 'land_area', 'bedrooms', 'bathrooms', 'total_rooms']].describe())\n",
    "\n",
    "# Verify no missing values\n",
    "print(f\"\\nMissing values after preprocessing: {df_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Summary:\n",
    "- Missing values are handled appropriately for each column type\n",
    "- Outliers in key numeric features are clipped to reasonable bounds\n",
    "- Property type column is removed since we're using the is_house binary feature\n",
    "- The preprocessing step ensures our data is clean and ready for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Now, let's create advanced features to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    # Basic log transformations for input features\n",
    "    df_engineered['log_living_area'] = np.log1p(df_engineered['living_area'])\n",
    "    df_engineered['log_land_area'] = np.log1p(df_engineered['land_area'])\n",
    "    \n",
    "    # Area ratio features\n",
    "    df_engineered['living_land_ratio'] = df_engineered['living_area'] / np.maximum(df_engineered['land_area'], 1)\n",
    "    df_engineered['sqm_per_room'] = df_engineered['living_area'] / np.maximum(df_engineered['total_rooms'], 1)\n",
    "    \n",
    "    # Room-related features and interactions\n",
    "    df_engineered['bed_bath'] = df_engineered['bedrooms'] * df_engineered['bathrooms']\n",
    "    df_engineered['bed_living'] = df_engineered['bedrooms'] * df_engineered['living_area']\n",
    "    df_engineered['bath_living'] = df_engineered['bathrooms'] * df_engineered['living_area']\n",
    "    df_engineered['rooms_per_living_area'] = df_engineered['total_rooms'] / np.maximum(df_engineered['living_area'], 1)\n",
    "    df_engineered['avg_room_size'] = df_engineered['living_area'] / np.maximum(df_engineered['total_rooms'], 1)\n",
    "    \n",
    "    # Polynomial features of key variables\n",
    "    df_engineered['living_area_squared'] = df_engineered['living_area'] ** 2\n",
    "    df_engineered['total_rooms_squared'] = df_engineered['total_rooms'] ** 2\n",
    "    df_engineered['bedrooms_squared'] = df_engineered['bedrooms'] ** 2\n",
    "    df_engineered['bathrooms_squared'] = df_engineered['bathrooms'] ** 2\n",
    "    \n",
    "    # Total amenities score\n",
    "    amenity_columns = [col for col in df_engineered.columns if col.startswith('has_')]\n",
    "    df_engineered['total_amenities'] = df_engineered[amenity_columns].sum(axis=1)\n",
    "    \n",
    "    # Create amenity groups\n",
    "    # Basic amenities\n",
    "    basic_amenities = ['has_parking', 'has_garage', 'has_interphone', 'has_kitchen_equipped']\n",
    "    if all(col in df_engineered.columns for col in basic_amenities):\n",
    "        df_engineered['basic_amenities'] = df_engineered[basic_amenities].sum(axis=1)\n",
    "    \n",
    "    # Comfort amenities\n",
    "    comfort_amenities = ['has_climatisation', 'has_central_heating', 'has_electric_heating', 'has_elevator']\n",
    "    if all(col in df_engineered.columns for col in comfort_amenities):\n",
    "        df_engineered['comfort_amenities'] = df_engineered[comfort_amenities].sum(axis=1)\n",
    "    \n",
    "    # Luxury amenities\n",
    "    luxury_amenities = ['has_pool', 'has_garden', 'has_terrace', 'has_sea_view']\n",
    "    if all(col in df_engineered.columns for col in luxury_amenities):\n",
    "        df_engineered['luxury_amenities'] = df_engineered[luxury_amenities].sum(axis=1)\n",
    "    \n",
    "    # Interactions with is_house\n",
    "    df_engineered['house_bedrooms'] = df_engineered['is_house'] * df_engineered['bedrooms']\n",
    "    df_engineered['house_living_area'] = df_engineered['is_house'] * df_engineered['living_area']\n",
    "    df_engineered['house_land_area'] = df_engineered['is_house'] * df_engineered['land_area']\n",
    "    \n",
    "    # Neighborhood-based features\n",
    "    if 'neighborhood_encoded' in df_engineered.columns and 'city_encoded' in df_engineered.columns:\n",
    "        # Create neighborhood-city interaction\n",
    "        df_engineered['neighborhood_city'] = df_engineered['neighborhood_encoded'] * df_engineered['city_encoded']\n",
    "    \n",
    "    # Additional advanced features\n",
    "    df_engineered['bed_bath_ratio'] = df_engineered['bedrooms'] / np.maximum(df_engineered['bathrooms'], 1)\n",
    "    df_engineered['bath_per_room'] = df_engineered['bathrooms'] / np.maximum(df_engineered['total_rooms'], 1)\n",
    "    \n",
    "    # Using is_house with amenities\n",
    "    df_engineered['house_with_garden'] = df_engineered['is_house'] * df_engineered['has_garden']\n",
    "    df_engineered['house_with_pool'] = df_engineered['is_house'] * df_engineered['has_pool']\n",
    "    \n",
    "    return df_engineered\n",
    "\n",
    "# Engineer features\n",
    "print(\"Engineering features...\")\n",
    "df_engineered = engineer_features(df_processed)\n",
    "\n",
    "# Print the engineered feature set\n",
    "print(f\"Engineered dataset shape: {df_engineered.shape}\")\n",
    "print(\"New features added:\")\n",
    "original_columns = set(df.columns)\n",
    "new_columns = set(df_engineered.columns) - original_columns\n",
    "print(list(new_columns))\n",
    "\n",
    "# Display summary stats for some new features\n",
    "new_feature_sample = ['log_living_area', 'bed_bath', 'total_amenities', 'luxury_amenities']\n",
    "display(df_engineered[new_feature_sample].describe())\n",
    "\n",
    "# Visualize some new features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Total amenities vs price\n",
    "sns.boxplot(x='total_amenities', y='price', data=df_engineered, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Price by Total Amenities')\n",
    "axes[0, 0].set_ylabel('Price')\n",
    "\n",
    "# Luxury amenities vs price\n",
    "sns.boxplot(x='luxury_amenities', y='price', data=df_engineered, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Price by Luxury Amenities')\n",
    "axes[0, 1].set_ylabel('Price')\n",
    "\n",
    "# bath_living vs price\n",
    "sns.scatterplot(x='bath_living', y='price', data=df_engineered, alpha=0.5, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Price vs Bathroom × Living Area')\n",
    "axes[1, 0].set_ylabel('Price')\n",
    "\n",
    "# house_land_area vs price\n",
    "sns.scatterplot(x='house_land_area', y='price', data=df_engineered, alpha=0.5, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Price vs House Land Area')\n",
    "axes[1, 1].set_ylabel('Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Summary:\n",
    "- Created 25 new engineered features including:\n",
    "  - Log transformations of area variables\n",
    "  - Area ratios and room interactions\n",
    "  - Polynomial terms for key variables\n",
    "  - Amenity groupings (basic, comfort, luxury)\n",
    "  - House-specific interactions\n",
    "  - Neighborhood and city interactions\n",
    "- The new features capture complex relationships in the data\n",
    "- Visualizations show strong relationships between new features and price\n",
    "- Particularly strong correlations with bath_living and house_land_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis\n",
    "\n",
    "Let's examine correlations between the features and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "correlation_matrix = df_engineered.corr()\n",
    "\n",
    "# Create correlation matrix with price\n",
    "price_correlations = correlation_matrix['price'].sort_values(ascending=False)\n",
    "\n",
    "# Display top positive correlations with price\n",
    "print(\"Top 15 features positively correlated with price:\")\n",
    "display(price_correlations.head(15))\n",
    "\n",
    "# Display top negative correlations with price\n",
    "print(\"\\nTop 5 features negatively correlated with price:\")\n",
    "display(price_correlations.tail(5))\n",
    "\n",
    "# Plot correlation heatmap for important features\n",
    "plt.figure(figsize=(16, 12))\n",
    "important_features = ['price', 'living_area', 'bedrooms', 'bathrooms', 'total_rooms', \n",
    "                      'is_house', 'land_area', 'bath_living', 'bed_bath', 'house_land_area',\n",
    "                      'total_amenities', 'luxury_amenities', 'log_living_area']\n",
    "\n",
    "sns.heatmap(df_engineered[important_features].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Key Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot scatterplot matrix for key features\n",
    "sns.pairplot(df_engineered[['price', 'living_area', 'bedrooms', 'bathrooms', 'is_house']], \n",
    "             height=2.5, plot_kws={'alpha': 0.6})\n",
    "plt.suptitle('Relationships Between Key Features', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis Insights:\n",
    "- Strong positive correlations between price and:\n",
    "  - bath_living (bathroom × living area interaction)\n",
    "  - living_area and living_area_squared\n",
    "  - house_land_area\n",
    "  - luxury amenities\n",
    "- The correlation matrix reveals multicollinearity between some features\n",
    "- Engineered features show stronger correlations with price than many original features\n",
    "- The pairplot shows both linear and non-linear relationships with price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Building\n",
    "\n",
    "Now, let's build and evaluate different models, including linear models, tree-based models, and a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "def build_and_evaluate_models(df, target_col='price', test_size=0.2, random_seed=42):\n",
    "    # Split data into features and target\n",
    "    X = df.drop([target_col], axis=1)\n",
    "    \n",
    "    # Logarithmic transformation of target\n",
    "    y = np.log1p(df[target_col])\n",
    "    \n",
    "    # Remove any text columns that might have slipped through\n",
    "    X = X.select_dtypes(exclude=['object'])\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Define preprocessing pipeline\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', RobustScaler())\n",
    "            ]), numeric_features)\n",
    "        ])\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Linear Regression': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LinearRegression())\n",
    "        ]),\n",
    "        \n",
    "        'Ridge (alpha=0.1)': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', Ridge(alpha=0.1, random_state=random_seed))\n",
    "        ]),\n",
    "        \n",
    "        'Lasso (alpha=0.001)': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', Lasso(alpha=0.001, max_iter=10000, random_state=random_seed))\n",
    "        ]),\n",
    "        \n",
    "        'XGBoost': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', xgb.XGBRegressor(\n",
    "                n_estimators=1000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=7,\n",
    "                min_child_weight=1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                gamma=0,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1,\n",
    "                random_state=random_seed,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ]),\n",
    "        \n",
    "        'Gradient Boosting': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', GradientBoostingRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=5,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                subsample=0.8,\n",
    "                max_features=0.8,\n",
    "                random_state=random_seed\n",
    "            ))\n",
    "        ]),\n",
    "        \n",
    "        'Random Forest': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                max_features='sqrt',\n",
    "                bootstrap=True,\n",
    "                random_state=random_seed,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Create a Stacking Regressor\n",
    "    stacking_model = StackingRegressor(\n",
    "        estimators=[\n",
    "            ('xgb', models['XGBoost']),\n",
    "            ('gb', models['Gradient Boosting']),\n",
    "            ('rf', models['Random Forest'])\n",
    "        ],\n",
    "        final_estimator=xgb.XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=3,\n",
    "            random_state=random_seed\n",
    "        ),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    models['Stacking Ensemble'] = stacking_model\n",
    "    \n",
    "    # Build Neural Network model\n",
    "    # Preprocess data for neural network\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Create and train neural network\n",
    "    print(\"\\nTraining Neural Network...\")\n",
    "    nn_model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train_preprocessed.shape[1],)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = nn_model.fit(\n",
    "        X_train_preprocessed, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Plot neural network training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Neural Network Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate Neural Network\n",
    "    y_pred_nn = nn_model.predict(X_test_preprocessed).flatten()\n",
    "    r2_nn = r2_score(y_test, y_pred_nn)\n",
    "    rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n",
    "    mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "    \n",
    "    # Transform NN predictions back to original scale\n",
    "    y_test_orig = np.expm1(y_test)\n",
    "    y_pred_nn_orig = np.expm1(y_pred_nn)\n",
    "    r2_nn_orig = r2_score(y_test_orig, y_pred_nn_orig)\n",
    "    rmse_nn_orig = np.sqrt(mean_squared_error(y_test_orig, y_pred_nn_orig))\n",
    "    \n",
    "    print(f\"Neural Network - Log Scale: R² = {r2_nn:.4f}, RMSE = {rmse_nn:.4f}, MAE = {mae_nn:.4f}\")\n",
    "    print(f\"Neural Network - Original Scale: R² = {r2_nn_orig:.4f}, RMSE = {rmse_nn_orig:.4f}\")\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    results = {\n",
    "        'Neural Network': {\n",
    "            'log_scale': {'r2': r2_nn, 'rmse': rmse_nn, 'mae': mae_nn},\n",
    "            'original_scale': {'r2': r2_nn_orig, 'rmse': rmse_nn_orig}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # For models except stacking, use cross-validation\n",
    "        if name != 'Stacking Ensemble':\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "            print(f\"Cross-validation R² scores: {cv_scores}\")\n",
    "            print(f\"Mean CV R² score: {cv_scores.mean():.4f}\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        # Transform predictions back to original scale\n",
    "        y_pred_orig = np.expm1(y_pred)\n",
    "        r2_orig = r2_score(y_test_orig, y_pred_orig)\n",
    "        rmse_orig = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig))\n",
    "        \n",
    "        results[name] = {\n",
    "            'log_scale': {'r2': r2, 'rmse': rmse, 'mae': mae},\n",
    "            'original_scale': {'r2': r2_orig, 'rmse': rmse_orig}\n",
    "        }\n",
    "        \n",
    "        print(f\"Test Results for {name}:\")\n",
    "        print(f\"Log Scale: R² = {r2:.4f}, RMSE = {rmse:.4f}, MAE = {mae:.4f}\")\n",
    "        print(f\"Original Scale: R² = {r2_orig:.4f}, RMSE = {rmse_orig:.4f}\")\n",
    "        \n",
    "        # Plot actual vs predicted for each model\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.title(f'Actual vs Predicted - {name} (Log Scale)')\n",
    "        plt.xlabel('Actual Log Price')\n",
    "        plt.ylabel('Predicted Log Price')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    # Find the best model\n",
    "    best_model_name = max(results, key=lambda x: results[x]['log_scale']['r2'])\n",
    "    \n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "    print(f\"Best model R² score: {results[best_model_name]['log_scale']['r2']:.4f}\")\n",
    "    \n",
    "    # Create a summary dataframe\n",
    "    summary = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'R² (log scale)': [results[model]['log_scale']['r2'] for model in results],\n",
    "        'RMSE (log scale)': [results[model]['log_scale']['rmse'] for model in results],\n",
    "        'MAE (log scale)': [results[model]['log_scale']['mae'] for model in results],\n",
    "        'R² (original scale)': [results[model]['original_scale']['r2'] for model in results],\n",
    "        'RMSE (original scale)': [results[model]['original_scale']['rmse'] for model in results]\n",
    "    })\n",
    "    \n",
    "    summary_sorted = summary.sort_values('R² (log scale)', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Plot model comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.barplot(x='R² (log scale)', y='Model', data=summary_sorted)\n",
    "    plt.title('Model Performance Comparison (R²)')\n",
    "    plt.xlabel('R² Score')\n",
    "    plt.grid(True, axis='x')\n",
    "    \n",
    "    # Add value labels to the bars\n",
    "    for i, v in enumerate(summary_sorted['R² (log scale)']):\n",
    "        ax.text(v + 0.01, i, f\"{v:.4f}\", va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return models, results, summary_sorted\n",
    "\n",
    "# Build and evaluate models\n",
    "print(\"Building and evaluating models...\")\n",
    "models, results, summary = build_and_evaluate_models(df_engineered)\n",
    "\n",
    "# Display summary of results\n",
    "print(\"\\nModel Performance Summary (sorted by R²):\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Summary:\n",
    "- We trained and evaluated 8 different models:\n",
    "  - Linear models: Linear Regression, Ridge, Lasso\n",
    "  - Tree-based models: Random Forest, Gradient Boosting, XGBoost\n",
    "  - Ensemble model: Stacking Ensemble\n",
    "  - Neural Network model\n",
    "- XGBoost achieved the highest R² score of approximately 0.76\n",
    "- Linear models performed significantly worse with R² around 0.62\n",
    "- Neural Network showed competitive performance but did not outperform XGBoost\n",
    "- The stacking ensemble did not improve over the base models, suggesting XGBoost already captures most patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis\n",
    "\n",
    "Let's examine which features are most important for our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, X):\n",
    "    # Get feature importances\n",
    "    if hasattr(model[-1], 'feature_importances_'):\n",
    "        importances = model[-1].feature_importances_\n",
    "    else:\n",
    "        print(\"This model doesn't support direct feature importance extraction\")\n",
    "        return None\n",
    "    \n",
    "    # Create a dataframe for visualization\n",
    "    feature_names = X.columns\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    ax = sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    \n",
    "    # Add value labels to the bars\n",
    "    for i, v in enumerate(importance_df['Importance'].head(20)):\n",
    "        ax.text(v + 0.001, i, f\"{v:.4f}\", va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Find the best model (XGBoost in this case)\n",
    "best_model_name = 'XGBoost'  # Based on previous results\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Analyze feature importance\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "X = df_engineered.drop(['price'], axis=1).select_dtypes(exclude=['object'])\n",
    "importance_df = analyze_feature_importance(best_model, X)\n",
    "\n",
    "if importance_df is not None:\n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    display(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Insights:\n",
    "- The top features driving price predictions are:\n",
    "  1. bath_living (bathrooms × living area) - 25.77%\n",
    "  2. living_area_squared - 17.99%\n",
    "  3. house_land_area - 4.60%\n",
    "  4. log_living_area - 3.08%\n",
    "  5. living_area - 2.85%\n",
    "- This shows that living area and its interactions have the strongest influence on price\n",
    "- The importance of house_land_area confirms that land is valued differently for houses vs apartments\n",
    "- Amenity features like has_pool also play a significant role\n",
    "- The neighborhood_city interaction feature is among the top predictors, showing location matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile key findings\n",
    "print(\"## Key Findings and Recommendations\")\n",
    "print(\"\\n1. Model Performance:\")\n",
    "display(summary.head(3))\n",
    "\n",
    "print(\"\\n2. Most Important Features:\")\n",
    "display(importance_df.head(5))\n",
    "\n",
    "print(\"\\n3. Recommendations for Improving the Model:\")\n",
    "recommendations = [\n",
    "    \"Feature engineering was highly effective - especially the bath_living feature\",\n",
    "    \"Consider trying more complex ensemble techniques like LightGBM or CatBoost\",\n",
    "    \"Experiment with different XGBoost parameter settings to fine-tune performance\",\n",
    "    \"Consider dimensionality reduction techniques to handle potential multicollinearity\",\n",
    "    \"Explore segmented models (e.g., separate models for houses and apartments)\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Recommendations:\n",
    "- XGBoost achieved the best performance with R² of 0.76 (log scale) and 0.74 (original scale)\n",
    "- The interaction between bathrooms and living area is the most powerful predictor of housing prices\n",
    "- Our feature engineering approach significantly improved model performance\n",
    "- Living area (and its transformations) dominates importance, along with house-specific land value\n",
    "- For further improvement:\n",
    "  1. Try advanced models like LightGBM or CatBoost\n",
    "  2. Fine-tune XGBoost hyperparameters through more extensive grid search\n",
    "  3. Consider building separate models for houses and apartments\n",
    "  4. Use dimensionality reduction to handle multicollinearity\n",
    "  5. Explore more location-based features if additional data becomes available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}